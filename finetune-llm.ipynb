{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa12ae1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "**Goal:** Fine-tune an LLM to predict execution times for CUDA kernel configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed7bd3f",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b789f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CSV_FILE = \"klaraptor_enriched_data.csv\"  # Path to your enriched data\n",
    "MODEL_NAME = \"gpt2\"  # or \"gpt2-medium\", \"microsoft/phi-2\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUTPUT_DIR = \"cuda_exec_time_predictor_llm\"\n",
    "\n",
    "# Training parameters\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Time filtering (optional - set to None to use all data)\n",
    "TIME_REGIME = None  # Options: \"fast\" (<1ms), \"medium\" (1-100ms), \"slow\" (>100ms), None (all)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Data file: {CSV_FILE}\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Time regime: {TIME_REGIME if TIME_REGIME else 'All'}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556875cb",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(row):\n",
    "    \"\"\"\n",
    "    Create a text prompt for execution time prediction\n",
    "    \n",
    "    Format:\n",
    "    ### Kernel: Convolution2D_kernel\n",
    "    N: 1024\n",
    "    dimensionality: 2D\n",
    "    compute_intensity: 10.50\n",
    "    Configuration: block_dims=(16, 16, 1), total_threads=256\n",
    "    ### Predicted Execution Time:\n",
    "    exec_time: 5.234 ms\n",
    "    log_time: 1.655\n",
    "    \"\"\"\n",
    "    total_threads = int(row['block_x'] * row['block_y'])\n",
    "    log_time = np.log(row['exec_time'])\n",
    "    \n",
    "    prompt = f\"\"\"### Kernel: {row['kernel_name']}\n",
    "N: {int(row['N'])}\n",
    "dimensionality: {int(row['dimensionality'])}D\n",
    "compute_intensity: {row['compute_intensity']:.2f}\n",
    "has_shared_memory: {row['has_shared_memory']}\n",
    "global_reads: {int(row['global_reads'])}\n",
    "global_writes: {int(row['global_writes'])}\n",
    "arithmetic_ops: {int(row['arithmetic_ops'])}\n",
    "memory_ops: {int(row['memory_ops'])}\n",
    "Configuration: block_dims=({int(row['block_x'])}, {int(row['block_y'])}, 1), total_threads={total_threads}\n",
    "\n",
    "### Predicted Execution Time:\n",
    "exec_time: {row['exec_time']:.6f} ms\n",
    "log_time: {log_time:.4f}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Test the prompt function\n",
    "test_row = {\n",
    "    'kernel_name': 'Convolution2D_kernel',\n",
    "    'N': 1024,\n",
    "    'dimensionality': 2,\n",
    "    'compute_intensity': 10.5,\n",
    "    'has_shared_memory': True,\n",
    "    'global_reads': 15,\n",
    "    'global_writes': 5,\n",
    "    'arithmetic_ops': 210,\n",
    "    'memory_ops': 20,\n",
    "    'block_x': 16,\n",
    "    'block_y': 16,\n",
    "    'exec_time': 5.234567\n",
    "}\n",
    "\n",
    "print(\"Example prompt:\")\n",
    "print(create_prompt(test_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333d61f",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd072191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# Handle column name variations\n",
    "column_mapping = {\n",
    "    'kernel': 'kernel_name',\n",
    "    'bx': 'block_x',\n",
    "    'by': 'block_y',\n",
    "    'bz': 'block_z'\n",
    "}\n",
    "df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n",
    "\n",
    "# Ensure required columns exist\n",
    "if 'block_x' not in df.columns:\n",
    "    df['block_x'] = df['bx'] if 'bx' in df.columns else 0\n",
    "if 'block_y' not in df.columns:\n",
    "    df['block_y'] = df['by'] if 'by' in df.columns else 0\n",
    "\n",
    "# Apply time filtering if specified\n",
    "if TIME_REGIME == \"fast\":\n",
    "    df = df[df['exec_time'] < 1.0]\n",
    "    print(f\"Filtering for FAST regime: exec_time < 1ms\")\n",
    "elif TIME_REGIME == \"medium\":\n",
    "    df = df[(df['exec_time'] >= 1.0) & (df['exec_time'] < 100.0)]\n",
    "    print(f\"Filtering for MEDIUM regime: 1ms ≤ exec_time < 100ms\")\n",
    "elif TIME_REGIME == \"slow\":\n",
    "    df = df[df['exec_time'] >= 100.0]\n",
    "    print(f\"Filtering for SLOW regime: exec_time ≥ 100ms\")\n",
    "\n",
    "print(f\"\\nLoaded {len(df):,} total configurations\")\n",
    "print(f\"Kernels: {df['kernel_name'].nunique()}\")\n",
    "print(f\"Data sizes (N): {sorted(df['N'].unique())}\")\n",
    "print(f\"Exec time range: {df['exec_time'].min():.6f} - {df['exec_time'].max():.6f} ms\")\n",
    "print(f\"Log exec time range: {np.log(df['exec_time'].min()):.2f} - {np.log(df['exec_time'].max()):.2f}\")\n",
    "\n",
    "if 'gpu' in df.columns:\n",
    "    print(f\"GPUs: {df['gpu'].nunique()} ({', '.join(df['gpu'].unique())})\")\n",
    "\n",
    "# Show sample\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4378d6",
   "metadata": {},
   "source": [
    "## 5. Sample Data for Training\n",
    "\n",
    "For LLMs, we'll use a subset of data (too much data can be slow). We'll sample diverse configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf256f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For LLM training, we'll use a stratified sample to keep training time reasonable\n",
    "# Sample proportionally from each kernel\n",
    "MAX_SAMPLES = 5000  # Adjust based on your GPU/CPU capacity\n",
    "\n",
    "if len(df) > MAX_SAMPLES:\n",
    "    print(f\"Sampling {MAX_SAMPLES} examples from {len(df):,} total...\")\n",
    "    \n",
    "    # Stratified sampling: same proportion from each kernel\n",
    "    df_sampled = df.groupby('kernel_name', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), MAX_SAMPLES // df['kernel_name'].nunique()), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"✓ Sampled {len(df_sampled):,} examples\")\n",
    "else:\n",
    "    df_sampled = df.copy()\n",
    "    print(f\"Using all {len(df_sampled):,} examples\")\n",
    "\n",
    "# Distribution stats\n",
    "print(f\"\\nSampled data statistics:\")\n",
    "print(f\"  Kernels: {df_sampled['kernel_name'].nunique()}\")\n",
    "print(f\"  Configs per kernel: {len(df_sampled) / df_sampled['kernel_name'].nunique():.1f} avg\")\n",
    "print(f\"  Exec time range: {df_sampled['exec_time'].min():.6f} - {df_sampled['exec_time'].max():.6f} ms\")\n",
    "\n",
    "# Show distribution by kernel\n",
    "df_sampled.groupby('kernel_name').agg({\n",
    "    'exec_time': ['count', 'min', 'max', 'mean']\n",
    "}).round(4).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04967a",
   "metadata": {},
   "source": [
    "## 6. Create Training Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompts for all sampled configs\n",
    "df_sampled['text'] = df_sampled.apply(create_prompt, axis=1)\n",
    "\n",
    "print(f\"Created {len(df_sampled)} training examples\")\n",
    "print(\"\\nExample training prompt:\")\n",
    "print(\"=\" * 70)\n",
    "print(df_sampled['text'].iloc[0])\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cc098",
   "metadata": {},
   "source": [
    "## 7. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set pad token (GPT-2 doesn't have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model (use float32 to avoid FP16 gradient issues)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(f\"✓ Model loaded: {MODEL_NAME}\")\n",
    "print(f\"  Parameters: {model.num_parameters():,}\")\n",
    "print(f\"  Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61373cbf",
   "metadata": {},
   "source": [
    "## 8. Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5da2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(df_optimal[['text']])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize texts for causal language modeling\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Split train/val (90/10)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"✓ Dataset prepared\")\n",
    "print(f\"  Training samples: {len(split_dataset['train'])}\")\n",
    "print(f\"  Validation samples: {len(split_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e552f",
   "metadata": {},
   "source": [
    "## 9. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),  # Use bfloat16 if supported\n",
    "    fp16=False,  # Disable fp16 to avoid gradient scaler issues\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"  # Disable wandb/tensorboard\n",
    ")\n",
    "\n",
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['test'],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0eb91f",
   "metadata": {},
   "source": [
    "## 10. Train the Model\n",
    "\n",
    "**Note:** This will take some time depending on your hardware.\n",
    "- CPU: 1-2 hours\n",
    "- GPU: 10-30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acf611",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759ee8c",
   "metadata": {},
   "source": [
    "## 11. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"✓ Model saved to: {OUTPUT_DIR}\")\n",
    "print(f\"  - Model weights: {OUTPUT_DIR}/pytorch_model.bin\")\n",
    "print(f\"  - Tokenizer: {OUTPUT_DIR}/tokenizer_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7456874d",
   "metadata": {},
   "source": [
    "## 12. Test Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b084946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_exec_time_with_llm(model, tokenizer, kernel_info, block_x, block_y):\n",
    "    \"\"\"\n",
    "    Use fine-tuned LLM to predict execution time for a given configuration\n",
    "    \n",
    "    Args:\n",
    "        kernel_info: dict with kernel characteristics\n",
    "        block_x, block_y: block dimensions to predict for\n",
    "        \n",
    "    Returns:\n",
    "        (predicted_exec_time, predicted_log_time, generated_text)\n",
    "    \"\"\"\n",
    "    total_threads = block_x * block_y\n",
    "    \n",
    "    # Create input prompt (without the output part)\n",
    "    prompt = f\"\"\"### Kernel: {kernel_info['kernel_name']}\n",
    "N: {kernel_info['N']}\n",
    "dimensionality: {kernel_info['dimensionality']}D\n",
    "compute_intensity: {kernel_info['compute_intensity']:.2f}\n",
    "has_shared_memory: {kernel_info['has_shared_memory']}\n",
    "global_reads: {kernel_info['global_reads']}\n",
    "global_writes: {kernel_info['global_writes']}\n",
    "arithmetic_ops: {kernel_info['arithmetic_ops']}\n",
    "memory_ops: {kernel_info['memory_ops']}\n",
    "Configuration: block_dims=({block_x}, {block_y}, 1), total_threads={total_threads}\n",
    "\n",
    "### Predicted Execution Time:\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the device the model is on\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.1,  # Low temperature for more deterministic output\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract exec_time from generated text\n",
    "    # Look for pattern like \"exec_time: 5.234 ms\"\n",
    "    match = re.search(r'exec_time:\\s*([\\d.]+)\\s*ms', generated_text)\n",
    "    if match:\n",
    "        exec_time = float(match.group(1))\n",
    "    else:\n",
    "        # Try to extract log_time instead\n",
    "        match_log = re.search(r'log_time:\\s*([-\\d.]+)', generated_text)\n",
    "        if match_log:\n",
    "            exec_time = np.exp(float(match_log.group(1)))\n",
    "        else:\n",
    "            print(\"⚠ Warning: Could not parse LLM output, using fallback estimate\")\n",
    "            # Rough estimate based on problem size\n",
    "            exec_time = (kernel_info['N'] ** kernel_info['dimensionality']) / 1e6\n",
    "    \n",
    "    log_time = np.log(exec_time)\n",
    "    \n",
    "    return exec_time, log_time, generated_text\n",
    "\n",
    "\n",
    "def find_optimal_config_with_llm(model, tokenizer, kernel_info, candidate_configs=None):\n",
    "    \"\"\"\n",
    "    Find optimal block configuration by testing multiple configs with LLM\n",
    "    \n",
    "    Returns:\n",
    "        (best_block_x, best_block_y, predicted_exec_time, all_predictions)\n",
    "    \"\"\"\n",
    "    # Generate candidate configs\n",
    "    if candidate_configs is None:\n",
    "        dim = kernel_info.get('dimensionality', 1)\n",
    "        if dim == 1:\n",
    "            candidate_configs = [(32, 1), (64, 1), (128, 1), (256, 1), (512, 1), (1024, 1)]\n",
    "        else:\n",
    "            candidate_configs = [\n",
    "                (8, 8), (16, 8), (16, 16), (32, 8), (32, 16), (32, 32),\n",
    "                (64, 4), (64, 8), (64, 16), (128, 4), (128, 8), (256, 4)\n",
    "            ]\n",
    "    \n",
    "    # Predict for all configs\n",
    "    predictions = []\n",
    "    for block_x, block_y in candidate_configs:\n",
    "        exec_time, log_time, _ = predict_exec_time_with_llm(\n",
    "            model, tokenizer, kernel_info, block_x, block_y\n",
    "        )\n",
    "        predictions.append({\n",
    "            'block_x': block_x,\n",
    "            'block_y': block_y,\n",
    "            'predicted_time': exec_time,\n",
    "            'log_time': log_time\n",
    "        })\n",
    "    \n",
    "    # Find best\n",
    "    best = min(predictions, key=lambda x: x['predicted_time'])\n",
    "    \n",
    "    return best['block_x'], best['block_y'], best['predicted_time'], predictions\n",
    "\n",
    "print(\"✓ Prediction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a34bce",
   "metadata": {},
   "source": [
    "## 13. Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions for different kernels and configurations\n",
    "test_cases = [\n",
    "    {\n",
    "        'kernel_name': 'Convolution2D_kernel',\n",
    "        'N': 1024,\n",
    "        'dimensionality': 2,\n",
    "        'compute_intensity': 10.5,\n",
    "        'has_shared_memory': True,\n",
    "        'global_reads': 15,\n",
    "        'global_writes': 5,\n",
    "        'arithmetic_ops': 210,\n",
    "        'memory_ops': 20\n",
    "    },\n",
    "    {\n",
    "        'kernel_name': 'mm2_kernel1',\n",
    "        'N': 2048,\n",
    "        'dimensionality': 2,\n",
    "        'compute_intensity': 15.2,\n",
    "        'has_shared_memory': True,\n",
    "        'global_reads': 20,\n",
    "        'global_writes': 10,\n",
    "        'arithmetic_ops': 304,\n",
    "        'memory_ops': 30\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"TEST PREDICTIONS - FINDING OPTIMAL CONFIGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for test_kernel in test_cases:\n",
    "    print(f\"\\nKernel: {test_kernel['kernel_name']}, N={test_kernel['N']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    bx, by, pred_time, all_preds = find_optimal_config_with_llm(\n",
    "        model, tokenizer, test_kernel\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Optimal configuration:\")\n",
    "    print(f\"    block_dims: ({bx}, {by}, 1)\")\n",
    "    print(f\"    predicted_time: {pred_time:.6f} ms\")\n",
    "    \n",
    "    print(f\"\\n  Top 5 configurations:\")\n",
    "    for pred in sorted(all_preds, key=lambda x: x['predicted_time'])[:5]:\n",
    "        print(f\"    ({pred['block_x']:4d}, {pred['block_y']:4d}): {pred['predicted_time']:.6f} ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffec956",
   "metadata": {},
   "source": [
    "## 14. Evaluate on Validation Set (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set - compare predicted vs actual execution times\n",
    "val_samples = df_sampled.sample(min(20, len(df_sampled)), random_state=42)\n",
    "\n",
    "predictions_list = []\n",
    "actuals_list = []\n",
    "\n",
    "print(\"Validation Set Predictions\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Kernel':<25} {'Config':<15} {'Predicted':<15} {'Actual':<15} {'Error %'}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for _, row in val_samples.iterrows():\n",
    "    kernel_info = {\n",
    "        'kernel_name': row['kernel_name'],\n",
    "        'N': int(row['N']),\n",
    "        'dimensionality': int(row['dimensionality']),\n",
    "        'compute_intensity': float(row['compute_intensity']),\n",
    "        'has_shared_memory': bool(row['has_shared_memory']),\n",
    "        'global_reads': int(row['global_reads']),\n",
    "        'global_writes': int(row['global_writes']),\n",
    "        'arithmetic_ops': int(row['arithmetic_ops']),\n",
    "        'memory_ops': int(row['memory_ops'])\n",
    "    }\n",
    "    \n",
    "    bx = int(row['block_x'])\n",
    "    by = int(row['block_y'])\n",
    "    \n",
    "    pred_time, _, _ = predict_exec_time_with_llm(model, tokenizer, kernel_info, bx, by)\n",
    "    actual_time = row['exec_time']\n",
    "    \n",
    "    error_pct = abs(pred_time - actual_time) / actual_time * 100\n",
    "    \n",
    "    predictions_list.append(pred_time)\n",
    "    actuals_list.append(actual_time)\n",
    "    \n",
    "    print(f\"{row['kernel_name'][:25]:<25} ({bx:3d},{by:3d})      \"\n",
    "          f\"{pred_time:>10.6f} ms  {actual_time:>10.6f} ms  {error_pct:>6.1f}%\")\n",
    "\n",
    "# Calculate metrics\n",
    "predictions_arr = np.array(predictions_list)\n",
    "actuals_arr = np.array(actuals_list)\n",
    "\n",
    "mae = np.mean(np.abs(predictions_arr - actuals_arr))\n",
    "mape = np.mean(np.abs((actuals_arr - predictions_arr) / actuals_arr)) * 100\n",
    "r2 = 1 - np.sum((actuals_arr - predictions_arr)**2) / np.sum((actuals_arr - actuals_arr.mean())**2)\n",
    "\n",
    "within_10 = (np.abs((actuals_arr - predictions_arr) / actuals_arr) < 0.1).mean() * 100\n",
    "within_20 = (np.abs((actuals_arr - predictions_arr) / actuals_arr) < 0.2).mean() * 100\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"Metrics:\")\n",
    "print(f\"  MAE: {mae:.6f} ms\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "print(f\"  R²: {r2:.4f}\")\n",
    "print(f\"  Within 10%: {within_10:.1f}%\")\n",
    "print(f\"  Within 20%: {within_20:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3b9dd",
   "metadata": {},
   "source": [
    "## 15. Summary\n",
    "\n",
    "You've successfully:\n",
    "1. ✓ Loaded the KLARAPTOR dataset with execution time data\n",
    "2. ✓ Sampled diverse configurations for training\n",
    "3. ✓ Created prompts that map (kernel features + block config) → execution time\n",
    "4. ✓ Fine-tuned an LLM to predict execution times\n",
    "5. ✓ Tested predictions and found optimal block configurations\n",
    "\n",
    "**Key Differences from Direct Block Prediction:**\n",
    "- **Input:** Kernel features + proposed block configuration\n",
    "- **Output:** Predicted execution time (in ms and log scale)\n",
    "- **Optimization:** Test multiple configs, choose one with lowest predicted time\n",
    "- **Advantage:** More data (all 15K+ configs vs. only ~200 optimal ones)\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different LLM sizes (gpt2-medium for better accuracy)\n",
    "- Experiment with time filtering (fast/medium/slow regimes)\n",
    "- Compare with neural network ensemble approach\n",
    "- Integrate with `grid_block_suggester.py` for end-to-end optimization\n",
    "\n",
    "**Comparison: LLM vs Neural Network**\n",
    "- **LLM:** More flexible, can handle text-based reasoning, slower inference\n",
    "- **Neural Network:** Faster inference, better for numerical prediction, easier to optimize\n",
    "- **Recommendation:** Use neural network ensemble for production (better performance)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

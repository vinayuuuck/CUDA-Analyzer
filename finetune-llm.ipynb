{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa12ae1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed7bd3f",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b789f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CSV_FILE = \"klaraptor_enriched_data.csv\"  # Path to your enriched data\n",
    "MODEL_NAME = \"gpt2\"  # or \"gpt2-medium\", \"microsoft/phi-2\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUTPUT_DIR = \"cuda_block_predictor_llm\"\n",
    "INCLUDE_GPU = False  # Set True to optimize per GPU, False for general optimization\n",
    "\n",
    "# Training parameters\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Data file: {CSV_FILE}\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556875cb",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(row):\n",
    "    \"\"\"\n",
    "    Create a text prompt for the LLM\n",
    "    \n",
    "    Format:\n",
    "    ### Kernel: vectorAdd\n",
    "    N: 4096\n",
    "    dimensionality: 1D\n",
    "    ...\n",
    "    ### Optimal Configuration:\n",
    "    block_dims: (256, 1, 1)\n",
    "    total_threads: 256\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"### Kernel: {row['kernel_name']}\n",
    "    N: {int(row['N'])}\n",
    "    dimensionality: {int(row['dimensionality'])}D\n",
    "    compute_intensity: {row['compute_intensity']:.2f}\n",
    "    has_shared_memory: {row['has_shared_memory']}\n",
    "    global_reads: {int(row['global_reads'])}\n",
    "    global_writes: {int(row['global_writes'])}\n",
    "    arithmetic_ops: {int(row['arithmetic_ops'])}\n",
    "    memory_ops: {int(row['memory_ops'])}\n",
    "\n",
    "    ### Optimal Configuration:\n",
    "    block_dims: ({int(row['block_x'])}, {int(row['block_y'])}, 1)\n",
    "    total_threads: {int(row['block_x'] * row['block_y'])}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Test the prompt function\n",
    "test_row = {\n",
    "    'kernel_name': 'vectorAdd',\n",
    "    'N': 4096,\n",
    "    'dimensionality': 1,\n",
    "    'compute_intensity': 12.5,\n",
    "    'has_shared_memory': False,\n",
    "    'global_reads': 10,\n",
    "    'global_writes': 5,\n",
    "    'arithmetic_ops': 125,\n",
    "    'memory_ops': 15,\n",
    "    'block_x': 256,\n",
    "    'block_y': 1\n",
    "}\n",
    "\n",
    "print(\"Example prompt:\")\n",
    "print(create_prompt(test_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333d61f",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd072191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# Handle column name variations\n",
    "column_mapping = {\n",
    "    'kernel': 'kernel_name',\n",
    "    'bx': 'block_x',\n",
    "    'by': 'block_y',\n",
    "    'bz': 'block_z'\n",
    "}\n",
    "df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n",
    "\n",
    "# Ensure required columns exist\n",
    "if 'block_x' not in df.columns:\n",
    "    df['block_x'] = df['bx'] if 'bx' in df.columns else 0\n",
    "if 'block_y' not in df.columns:\n",
    "    df['block_y'] = df['by'] if 'by' in df.columns else 0\n",
    "\n",
    "print(f\"Loaded {len(df)} total configurations\")\n",
    "print(f\"Kernels: {df['kernel_name'].nunique()}\")\n",
    "print(f\"Data sizes (N): {sorted(df['N'].unique())}\")\n",
    "if 'gpu' in df.columns:\n",
    "    print(f\"GPUs: {df['gpu'].nunique()} ({', '.join(df['gpu'].unique())})\")\n",
    "\n",
    "# Show sample\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4378d6",
   "metadata": {},
   "source": [
    "## 5. Find Optimal Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf256f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by kernel, N, and optionally GPU\n",
    "group_cols = ['kernel_name', 'N']\n",
    "if INCLUDE_GPU and 'gpu' in df.columns:\n",
    "    group_cols.append('gpu')\n",
    "\n",
    "# Find optimal configs (min exec time for each group)\n",
    "print(\"Finding optimal configurations...\")\n",
    "optimal_rows = []\n",
    "\n",
    "for group_key, group in df.groupby(group_cols):\n",
    "    # Find row with minimum execution time\n",
    "    best_idx = group['exec_time'].idxmin()\n",
    "    best_row = group.loc[best_idx].copy()\n",
    "    \n",
    "    # Add some context: how much better is this than average?\n",
    "    avg_time = group['exec_time'].mean()\n",
    "    best_time = best_row['exec_time']\n",
    "    speedup = avg_time / best_time\n",
    "    \n",
    "    best_row['speedup_vs_avg'] = speedup\n",
    "    best_row['configs_tested'] = len(group)\n",
    "    \n",
    "    optimal_rows.append(best_row)\n",
    "\n",
    "df_optimal = pd.DataFrame(optimal_rows)\n",
    "\n",
    "print(f\"✓ Found {len(df_optimal)} optimal configurations\")\n",
    "print(f\"  Configs per kernel: {len(df_optimal) / df['kernel_name'].nunique():.1f} avg\")\n",
    "print(f\"  Avg speedup vs random config: {df_optimal['speedup_vs_avg'].mean():.2f}x\")\n",
    "\n",
    "# Show sample optimal configs\n",
    "df_optimal[['kernel_name', 'N', 'block_x', 'block_y', 'exec_time', 'speedup_vs_avg']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04967a",
   "metadata": {},
   "source": [
    "## 6. Create Training Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompts for all optimal configs\n",
    "df_optimal['text'] = df_optimal.apply(create_prompt, axis=1)\n",
    "\n",
    "print(f\"Created {len(df_optimal)} training examples\")\n",
    "print(\"\\nExample training prompt:\")\n",
    "print(\"=\" * 60)\n",
    "print(df_optimal['text'].iloc[0])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cc098",
   "metadata": {},
   "source": [
    "## 7. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set pad token (GPT-2 doesn't have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model (use float32 to avoid FP16 gradient issues)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(f\"✓ Model loaded: {MODEL_NAME}\")\n",
    "print(f\"  Parameters: {model.num_parameters():,}\")\n",
    "print(f\"  Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61373cbf",
   "metadata": {},
   "source": [
    "## 8. Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5da2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(df_optimal[['text']])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize texts for causal language modeling\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Split train/val (90/10)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"✓ Dataset prepared\")\n",
    "print(f\"  Training samples: {len(split_dataset['train'])}\")\n",
    "print(f\"  Validation samples: {len(split_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e552f",
   "metadata": {},
   "source": [
    "## 9. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),  # Use bfloat16 if supported\n",
    "    fp16=False,  # Disable fp16 to avoid gradient scaler issues\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"  # Disable wandb/tensorboard\n",
    ")\n",
    "\n",
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['test'],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0eb91f",
   "metadata": {},
   "source": [
    "## 10. Train the Model\n",
    "\n",
    "**Note:** This will take some time depending on your hardware.\n",
    "- CPU: 1-2 hours\n",
    "- GPU: 10-30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acf611",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759ee8c",
   "metadata": {},
   "source": [
    "## 11. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"✓ Model saved to: {OUTPUT_DIR}\")\n",
    "print(f\"  - Model weights: {OUTPUT_DIR}/pytorch_model.bin\")\n",
    "print(f\"  - Tokenizer: {OUTPUT_DIR}/tokenizer_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7456874d",
   "metadata": {},
   "source": [
    "## 12. Test Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b084946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_llm(model, tokenizer, kernel_info: dict):\n",
    "    \"\"\"\n",
    "    Use fine-tuned LLM to predict block configuration\n",
    "    \n",
    "    Args:\n",
    "        kernel_info: dict with kernel characteristics\n",
    "        \n",
    "    Returns:\n",
    "        (block_x, block_y, total_threads)\n",
    "    \"\"\"\n",
    "    # Create input prompt (without the output part)\n",
    "    prompt = f\"\"\"### Kernel: {kernel_info['kernel_name']}\n",
    "    N: {kernel_info['N']}\n",
    "    dimensionality: {kernel_info['dimensionality']}D\n",
    "    compute_intensity: {kernel_info['compute_intensity']:.2f}\n",
    "    has_shared_memory: {kernel_info['has_shared_memory']}\n",
    "    global_reads: {kernel_info['global_reads']}\n",
    "    global_writes: {kernel_info['global_writes']}\n",
    "    arithmetic_ops: {kernel_info['arithmetic_ops']}\n",
    "    memory_ops: {kernel_info['memory_ops']}\n",
    "\n",
    "    ### Optimal Configuration:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the device the model is on\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.1,  # Low temperature for more deterministic output\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract block dims from generated text\n",
    "    # Look for pattern like \"block_dims: (256, 1, 1)\"\n",
    "    match = re.search(r'block_dims:\\s*\\((\\d+),\\s*(\\d+),\\s*(\\d+)\\)', generated_text)\n",
    "    if match:\n",
    "        block_x = int(match.group(1))\n",
    "        block_y = int(match.group(2))\n",
    "        total_threads = block_x * block_y\n",
    "        return block_x, block_y, total_threads, generated_text\n",
    "    else:\n",
    "        # Fallback to default\n",
    "        print(\"⚠ Warning: Could not parse LLM output, using default\")\n",
    "        return 256, 1, 256, generated_text\n",
    "\n",
    "print(\"✓ Prediction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a34bce",
   "metadata": {},
   "source": [
    "## 13. Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a few different kernels\n",
    "test_cases = [\n",
    "    {\n",
    "        'kernel_name': 'matrixMul',\n",
    "        'N': 2048,\n",
    "        'dimensionality': 2,\n",
    "        'compute_intensity': 15.2,\n",
    "        'has_shared_memory': True,\n",
    "        'global_reads': 20,\n",
    "        'global_writes': 10,\n",
    "        'arithmetic_ops': 304,\n",
    "        'memory_ops': 30\n",
    "    },\n",
    "    {\n",
    "        'kernel_name': 'vectorAdd',\n",
    "        'N': 1048576,\n",
    "        'dimensionality': 1,\n",
    "        'compute_intensity': 3.5,\n",
    "        'has_shared_memory': False,\n",
    "        'global_reads': 2,\n",
    "        'global_writes': 1,\n",
    "        'arithmetic_ops': 7,\n",
    "        'memory_ops': 3\n",
    "    },\n",
    "    {\n",
    "        'kernel_name': 'Convolution2D_kernel',\n",
    "        'N': 4096,\n",
    "        'dimensionality': 2,\n",
    "        'compute_intensity': 8.3,\n",
    "        'has_shared_memory': True,\n",
    "        'global_reads': 15,\n",
    "        'global_writes': 5,\n",
    "        'arithmetic_ops': 166,\n",
    "        'memory_ops': 20\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"TEST PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for test_kernel in test_cases:\n",
    "    print(f\"\\nKernel: {test_kernel['kernel_name']}, N={test_kernel['N']}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    bx, by, total, full_output = predict_with_llm(model, tokenizer, test_kernel)\n",
    "    \n",
    "    print(f\"Predicted configuration:\")\n",
    "    print(f\"  block_dims: ({bx}, {by}, 1)\")\n",
    "    print(f\"  total_threads: {total}\")\n",
    "    \n",
    "    # Show part of the generated text\n",
    "    output_part = full_output.split(\"### Optimal Configuration:\")[-1][:200]\n",
    "    print(f\"\\nGenerated output:\")\n",
    "    print(output_part.strip())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffec956",
   "metadata": {},
   "source": [
    "## 14. Evaluate on Validation Set (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some validation examples and test predictions\n",
    "val_samples = df_optimal.sample(min(10, len(df_optimal)), random_state=42)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(\"Validation Set Predictions\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Kernel':<20} {'N':<8} {'Predicted':<20} {'Actual':<20} {'Match'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for _, row in val_samples.iterrows():\n",
    "    kernel_info = {\n",
    "        'kernel_name': row['kernel_name'],\n",
    "        'N': int(row['N']),\n",
    "        'dimensionality': int(row['dimensionality']),\n",
    "        'compute_intensity': float(row['compute_intensity']),\n",
    "        'has_shared_memory': bool(row['has_shared_memory']),\n",
    "        'global_reads': int(row['global_reads']),\n",
    "        'global_writes': int(row['global_writes']),\n",
    "        'arithmetic_ops': int(row['arithmetic_ops']),\n",
    "        'memory_ops': int(row['memory_ops'])\n",
    "    }\n",
    "    \n",
    "    bx_pred, by_pred, total_pred, _ = predict_with_llm(model, tokenizer, kernel_info)\n",
    "    bx_actual = int(row['block_x'])\n",
    "    by_actual = int(row['block_y'])\n",
    "    \n",
    "    match = \"✓\" if (bx_pred == bx_actual and by_pred == by_actual) else \"✗\"\n",
    "    if match == \"✓\":\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "    print(f\"{row['kernel_name'][:20]:<20} {int(row['N']):<8} \"\n",
    "          f\"({bx_pred:4d}, {by_pred:4d})     \"\n",
    "          f\"({bx_actual:4d}, {by_actual:4d})     {match}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Accuracy: {correct}/{total} = {100*correct/total:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3b9dd",
   "metadata": {},
   "source": [
    "## 15. Summary\n",
    "\n",
    "You've successfully:\n",
    "1. ✓ Loaded and prepared the KLARAPTOR dataset\n",
    "2. ✓ Found optimal configurations for each kernel/size combination\n",
    "3. ✓ Created training prompts in text format\n",
    "4. ✓ Fine-tuned an LLM to predict block configurations\n",
    "5. ✓ Tested predictions on new kernel configurations\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different model sizes (gpt2-medium, phi-2, TinyLlama)\n",
    "- Experiment with training parameters (epochs, learning rate)\n",
    "- Add more features to the prompt (GPU architecture, compute capability)\n",
    "- Integrate with your grid_block_suggester.py for inference"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
